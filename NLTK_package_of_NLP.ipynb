{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **Install nltk package**"
      ],
      "metadata": {
        "id": "BfjgRWUee0w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onhnDn5te6Kw",
        "outputId": "bdeb56e9-1612-42d9-80b7-1dc4934a551e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "import nltk\n",
        "nltk.download('punkt') # split a text into sentences based on punctuation marks like periods, question marks, etc.\n",
        "import nltk.corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gyj-685fiog",
        "outputId": "87f7fe77-41cf-4d41-df1e-dd11c43f4899"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "IHWqsp4rfn4R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#string\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\""
      ],
      "metadata": {
        "id": "pIx55I9Dftox"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing\n",
        "\n",
        "text_tokens = word_tokenize(text)\n",
        "text_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6yPIKO5gnnR",
        "outputId": "d60a7f6e-a1c9-4855-fb42-bf5c2de27617"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There',\n",
              " 'are',\n",
              " 'multiple',\n",
              " 'ways',\n",
              " 'we',\n",
              " 'can',\n",
              " 'perform',\n",
              " 'tokenization',\n",
              " 'on',\n",
              " 'given',\n",
              " 'text',\n",
              " 'data',\n",
              " '.',\n",
              " 'We',\n",
              " 'can',\n",
              " 'choose',\n",
              " 'any',\n",
              " 'method',\n",
              " 'based',\n",
              " 'on',\n",
              " 'langauge',\n",
              " ',',\n",
              " 'library',\n",
              " 'and',\n",
              " 'purpose',\n",
              " 'of',\n",
              " 'modeling',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the type and number of tokens\n",
        "type(text_tokens), len(text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWvdl22Hhk8I",
        "outputId": "92990a10-9f90-4564-9a33-1c520d76385e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency of token\n",
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()  #instance of FreqDist"
      ],
      "metadata": {
        "id": "rNvtTaWMjA4z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_tokens:\n",
        "  fdist[i] = fdist[i]+1\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDXSZoIkjevX",
        "outputId": "b433aa3e-b2cb-40aa-db8f-b19d17457a1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'can': 2, 'on': 2, '.': 2, 'There': 1, 'are': 1, 'multiple': 1, 'ways': 1, 'we': 1, 'perform': 1, 'tokenization': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find 10 most common tokens\n",
        "fdist_top10 = fdist.most_common(10)\n",
        "fdist_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AFO6AT0kUpU",
        "outputId": "5c1bf9f5-fcdd-4bf5-d456-6352201c9b0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('can', 2),\n",
              " ('on', 2),\n",
              " ('.', 2),\n",
              " ('There', 1),\n",
              " ('are', 1),\n",
              " ('multiple', 1),\n",
              " ('ways', 1),\n",
              " ('we', 1),\n",
              " ('perform', 1),\n",
              " ('tokenization', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language Modelling**"
      ],
      "metadata": {
        "id": "umsINLQUmQve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in bigram\n",
        "\n",
        "list(nltk.bigrams(text_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4Yu4v1Ck_jB",
        "outputId": "50f2c955-3853-4666-ae8f-188e1a964fda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('There', 'are'),\n",
              " ('are', 'multiple'),\n",
              " ('multiple', 'ways'),\n",
              " ('ways', 'we'),\n",
              " ('we', 'can'),\n",
              " ('can', 'perform'),\n",
              " ('perform', 'tokenization'),\n",
              " ('tokenization', 'on'),\n",
              " ('on', 'given'),\n",
              " ('given', 'text'),\n",
              " ('text', 'data'),\n",
              " ('data', '.'),\n",
              " ('.', 'We'),\n",
              " ('We', 'can'),\n",
              " ('can', 'choose'),\n",
              " ('choose', 'any'),\n",
              " ('any', 'method'),\n",
              " ('method', 'based'),\n",
              " ('based', 'on'),\n",
              " ('on', 'langauge'),\n",
              " ('langauge', ','),\n",
              " (',', 'library'),\n",
              " ('library', 'and'),\n",
              " ('and', 'purpose'),\n",
              " ('purpose', 'of'),\n",
              " ('of', 'modeling'),\n",
              " ('modeling', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in trigram\n",
        "\n",
        "list(nltk.trigrams(text_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQZ-bdLIlyIJ",
        "outputId": "16e151ce-94cb-48cc-f6d9-5f0c8a1e3374"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('There', 'are', 'multiple'),\n",
              " ('are', 'multiple', 'ways'),\n",
              " ('multiple', 'ways', 'we'),\n",
              " ('ways', 'we', 'can'),\n",
              " ('we', 'can', 'perform'),\n",
              " ('can', 'perform', 'tokenization'),\n",
              " ('perform', 'tokenization', 'on'),\n",
              " ('tokenization', 'on', 'given'),\n",
              " ('on', 'given', 'text'),\n",
              " ('given', 'text', 'data'),\n",
              " ('text', 'data', '.'),\n",
              " ('data', '.', 'We'),\n",
              " ('.', 'We', 'can'),\n",
              " ('We', 'can', 'choose'),\n",
              " ('can', 'choose', 'any'),\n",
              " ('choose', 'any', 'method'),\n",
              " ('any', 'method', 'based'),\n",
              " ('method', 'based', 'on'),\n",
              " ('based', 'on', 'langauge'),\n",
              " ('on', 'langauge', ','),\n",
              " ('langauge', ',', 'library'),\n",
              " (',', 'library', 'and'),\n",
              " ('library', 'and', 'purpose'),\n",
              " ('and', 'purpose', 'of'),\n",
              " ('purpose', 'of', 'modeling'),\n",
              " ('of', 'modeling', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in ngram\n",
        "\n",
        "list(nltk.ngrams(text_tokens,5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8x7ID8Il9Ck",
        "outputId": "b2aa7f63-0af4-4d9d-e14a-4da71a35b0f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('There', 'are', 'multiple', 'ways', 'we'),\n",
              " ('are', 'multiple', 'ways', 'we', 'can'),\n",
              " ('multiple', 'ways', 'we', 'can', 'perform'),\n",
              " ('ways', 'we', 'can', 'perform', 'tokenization'),\n",
              " ('we', 'can', 'perform', 'tokenization', 'on'),\n",
              " ('can', 'perform', 'tokenization', 'on', 'given'),\n",
              " ('perform', 'tokenization', 'on', 'given', 'text'),\n",
              " ('tokenization', 'on', 'given', 'text', 'data'),\n",
              " ('on', 'given', 'text', 'data', '.'),\n",
              " ('given', 'text', 'data', '.', 'We'),\n",
              " ('text', 'data', '.', 'We', 'can'),\n",
              " ('data', '.', 'We', 'can', 'choose'),\n",
              " ('.', 'We', 'can', 'choose', 'any'),\n",
              " ('We', 'can', 'choose', 'any', 'method'),\n",
              " ('can', 'choose', 'any', 'method', 'based'),\n",
              " ('choose', 'any', 'method', 'based', 'on'),\n",
              " ('any', 'method', 'based', 'on', 'langauge'),\n",
              " ('method', 'based', 'on', 'langauge', ','),\n",
              " ('based', 'on', 'langauge', ',', 'library'),\n",
              " ('on', 'langauge', ',', 'library', 'and'),\n",
              " ('langauge', ',', 'library', 'and', 'purpose'),\n",
              " (',', 'library', 'and', 'purpose', 'of'),\n",
              " ('library', 'and', 'purpose', 'of', 'modeling'),\n",
              " ('and', 'purpose', 'of', 'modeling', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:** It is process of reducing a word to it's word stem by cutting off beginning or the end.\n",
        "\n",
        "studies->studi\n",
        "\n",
        "giving-> giv\n",
        "\n",
        "buying-> buy"
      ],
      "metadata": {
        "id": "7-BGtmncolj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer() #instance of PorterStemmer"
      ],
      "metadata": {
        "id": "CT3RhmscmFka"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pst.stem('modeling'),pst.stem('tokenization')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTLaw2w_6JTE",
        "outputId": "c2e168a3-bdb7-4d62-b658-ded634164541"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model', 'token')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatizing :** reducing words into thier lemma and dictionary.\n",
        "\n",
        "studies->study\n",
        "\n",
        "giving-> give\n",
        "\n",
        "buying-> buy"
      ],
      "metadata": {
        "id": "RYE9vPS64hAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part of speech tagging :**  words are categorized into 8 parts.\n",
        "\n",
        "noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.\n",
        "\n",
        "**example:** She loves pizza.\n",
        "\n",
        "she -> Common pronoun\n",
        "\n",
        "pizza -> noun\n",
        "\n",
        "loves -> verb"
      ],
      "metadata": {
        "id": "b5DEl_WaCXvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pos\n",
        "from nltk import pos_tag\n",
        "The_wolf='''There once was a boy who grew bored while watching over the village sheep. He wanted to make things more exciting. So, he yelled out that he saw a wolf chasing the sheep.'''"
      ],
      "metadata": {
        "id": "BhvzyC3Z_yew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The_wolf_tokens = word_tokenize(The_wolf)\n",
        "The_wolf_tokens"
      ],
      "metadata": {
        "id": "TWIsQzWJSfed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in The_wolf_tokens:\n",
        "  print(nltk.pos_tag([i]))\n"
      ],
      "metadata": {
        "id": "cw70v8ioF8s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[('There', 'EX')] existential\n",
        "\n",
        "[('once', 'RB')] adverb\n",
        "\n",
        "'WP': Wh-pronoun (e.g., \"who\")\n",
        "\n",
        "'IN': Preposition (e.g., \"while\", \"over\", \"out\", \"that\")\n",
        "\n",
        "'VBD': Verb, past tense (e.g., \"was\", \"grew\", \"wanted\")\n",
        "\n",
        "'PRP': Personal pronoun (e.g., \"He\", \"he\")"
      ],
      "metadata": {
        "id": "ETUDCNALUKQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition :**  It is a process in natural language processing that identifies and classifies named entities in a text into predefined categories such as **person names, organizations, locations, dates**, etc."
      ],
      "metadata": {
        "id": "pkzUodkBbPw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n"
      ],
      "metadata": {
        "id": "OoXRkIXlRW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barack=\"Barack Obama was born in Hawaii. He was the president of the United States.\""
      ],
      "metadata": {
        "id": "U-re9YDBcLdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "barack_tokens = word_tokenize(barack)\n",
        "barack_tokens"
      ],
      "metadata": {
        "id": "m4wYglsWcPJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pos\n",
        "barack_pos = nltk.pos_tag(barack_tokens)\n",
        "barack_pos"
      ],
      "metadata": {
        "id": "eDrCIUQNcUdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "Pc3lqV5sclSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barack_ner=ne_chunk(run_pos)\n",
        "print(run_ner)"
      ],
      "metadata": {
        "id": "BKkiJsT0c_G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPE:** location"
      ],
      "metadata": {
        "id": "2fTKrdVsjhuM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TYbLXgBiaVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}